{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import cameras\n",
    "import data_utils\n",
    "import linear_model\n",
    "import procrustes\n",
    "import viz\n",
    "import glob\n",
    "import cdflib\n",
    "sys.argv = sys.argv[:1]\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import data_process as data_process\n",
    "\n",
    "import json \n",
    "from model import LinearModel, OptunaModel, weight_init\n",
    "import torch.nn as nn\n",
    "import utils as utils\n",
    "\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname('__file__'), \"progress\"))\n",
    "\n",
    "from progress.bar import Bar as Bar\n",
    "\n",
    "# Load Human3.6M Skeleton\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pickle\n",
    "\n",
    "tf.app.flags.DEFINE_string(\"action\",\"All\", \"The action to train on. 'All' means all the actions\")\n",
    "\n",
    "# Directories\n",
    "tf.app.flags.DEFINE_string(\"cameras_path\",\"../data/h36m/metadata.xml\", \"File with h36m metadata, including cameras\")\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECT_IDS = [1,5,6,7,8,9,11]\n",
    "this_file = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "#Load metadata.xml camera\n",
    "rcams = cameras.load_cameras(os.path.join(this_file, FLAGS.cameras_path), SUBJECT_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Joints in H3.6M -- data has 32 joints, but only 17 that move; these are the indices.\n",
    "H36M_NAMES = ['']*32\n",
    "H36M_NAMES[0]  = 'Hip'\n",
    "H36M_NAMES[1]  = 'RHip'\n",
    "H36M_NAMES[2]  = 'RKnee'\n",
    "H36M_NAMES[3]  = 'RFoot'\n",
    "H36M_NAMES[6]  = 'LHip'\n",
    "H36M_NAMES[7]  = 'LKnee'\n",
    "H36M_NAMES[8]  = 'LFoot'\n",
    "H36M_NAMES[12] = 'Spine'\n",
    "H36M_NAMES[13] = 'Thorax'\n",
    "H36M_NAMES[14] = 'Neck/Nose'\n",
    "H36M_NAMES[15] = 'Head'\n",
    "H36M_NAMES[17] = 'LShoulder'\n",
    "H36M_NAMES[18] = 'LElbow'\n",
    "H36M_NAMES[19] = 'LWrist'\n",
    "H36M_NAMES[25] = 'RShoulder'\n",
    "H36M_NAMES[26] = 'RElbow'\n",
    "H36M_NAMES[27] = 'RWrist'\n",
    "\n",
    "index_alphapose={\n",
    "    # Use 17 skeleton point\n",
    "    \"Nose\": 0,\n",
    "    \"RShoulder\": 6,\n",
    "    \"RElbow\": 8,\n",
    "    \"RWrist\": 10,\n",
    "    \"LShoulder\": 5,\n",
    "    \"LElbow\": 7,\n",
    "    \"LWrist\": 9,\n",
    "    \"RHip\": 12,\n",
    "    \"RKnee\": 14,\n",
    "    \"RAnkle\": 16,\n",
    "    \"LHip\": 11,\n",
    "    \"LKnee\": 13,\n",
    "    \"LAnkle\": 15,\n",
    "    \"REye\": 2,\n",
    "    \"LEye\": 1,\n",
    "    \"REar\": 4,\n",
    "    \"LEar\": 3\n",
    "}\n",
    "\n",
    "index_mapping={\n",
    "# Alpha Pose to Human 3.6M\n",
    "\"Hip\": [20, 0],\n",
    "\"RHip\": [12,1],\n",
    "\"RKnee\": [14,2],\n",
    "\"RFoot\": [16,3],\n",
    "\"LHip\": [11,6],\n",
    "\"LKnee\": [13,7],\n",
    "\"LFoot\": [15,8],\n",
    "\"Spine\": [19,12],\n",
    "\"Thorax\": [18,13],\n",
    "# \"Nose\": [14,0],\n",
    "\"Head\": [17,15],\n",
    "\"LShoulder\": [5,17],\n",
    "\"LElbow\": [7,18],\n",
    "\"LWrist\": [9,19],\n",
    "\"RShoulder\": [6,25],\n",
    "\"RElbow\": [8,26],\n",
    "\"RWrist\": [10,27]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "index_mapping_nose={\n",
    "# Alpha Pose to Human 3.6M\n",
    "\"Hip\": [20, 0],\n",
    "\"RHip\": [12,1],\n",
    "\"RKnee\": [14,2],\n",
    "\"RFoot\": [16,3],\n",
    "\"LHip\": [11,6],\n",
    "\"LKnee\": [13,7],\n",
    "\"LFoot\": [15,8],\n",
    "\"Spine\": [19,12],\n",
    "\"Thorax\": [18,13],\n",
    "\"Nose\": [0,14],\n",
    "\"Head\": [17,15],\n",
    "\"LShoulder\": [5,17],\n",
    "\"LElbow\": [7,18],\n",
    "\"LWrist\": [9,19],\n",
    "\"RShoulder\": [6,25],\n",
    "\"RElbow\": [8,26],\n",
    "\"RWrist\": [10,27]\n",
    "}\n",
    "\n",
    "\n",
    "def data_converter(data):\n",
    "    data=data['keypoints']\n",
    "    keypoints=[]\n",
    "    kp_score=[]\n",
    "    for a in range (0,len(data)):\n",
    "        score=[]\n",
    "        if ((a+3)%3==0):\n",
    "            keypoints.append(data[a])\n",
    "            keypoints.append(data[a+1])\n",
    "        elif((a+1)%3==0):\n",
    "            score=data[a]\n",
    "            kp_score.append(score)\n",
    "\n",
    "    return keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def count_head(alpha_pose):\n",
    "    x = (alpha_pose[index_alphapose['LEar']*2]+alpha_pose[index_alphapose['REar']*2])/2\n",
    "    y = (alpha_pose[index_alphapose['LEar']*2+1]+alpha_pose[index_alphapose['REar']*2+1])/2\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def count_thorax(alpha_pose):\n",
    "    x = (alpha_pose[index_alphapose['LShoulder']*2]+alpha_pose[index_alphapose['RShoulder']*2])/2\n",
    "    y = (alpha_pose[index_alphapose['LShoulder']*2+1]+alpha_pose[index_alphapose['RShoulder']*2+1])/2\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def count_spine(alpha_pose):\n",
    "    hip_x,hip_y=count_hip(alpha_pose)\n",
    "    thorax_x,thorax_y=count_thorax(alpha_pose)\n",
    "    x = (hip_x+thorax_x)/2\n",
    "    y = (hip_y+thorax_y)/2\n",
    "    return x,y\n",
    "\n",
    "def count_hip(alpha_pose):\n",
    "    x = (alpha_pose[index_alphapose['LHip']*2]+alpha_pose[index_alphapose['RHip']*2])/2\n",
    "    y = (alpha_pose[index_alphapose['LHip']*2+1]+alpha_pose[index_alphapose['RHip']*2+1])/2\n",
    "    return x,y\n",
    "\n",
    "def add_features(alpha_pose):\n",
    "    #Count Head\n",
    "    head_x,head_y=count_head(alpha_pose)\n",
    "    alpha_pose=np.append(alpha_pose,(head_x,head_y))\n",
    "    \n",
    "    #Count Thorax\n",
    "    thorax_x,thorax_y=count_thorax(alpha_pose)\n",
    "    alpha_pose=np.append(alpha_pose,(thorax_x,thorax_y))\n",
    " \n",
    "    \n",
    "    #Count Spine\n",
    "    spine_x,spine_y=count_spine(alpha_pose)\n",
    "    alpha_pose=np.append(alpha_pose,(spine_x,spine_y))\n",
    "    \n",
    "    #Count Hip\n",
    "    hip_x,hip_y=count_hip(alpha_pose)\n",
    "    alpha_pose=np.append(alpha_pose,(hip_x,hip_y))\n",
    "    \n",
    "    return alpha_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is not includding nose for 2D to 3D\n",
    "def map_alpha_to_human(alpha_pose):\n",
    "    alpha_pose=add_features(alpha_pose)\n",
    "    temp_list = [None] * 64\n",
    "    for a,b in index_mapping.items():\n",
    "        temp_list[b[1]*2]=alpha_pose[b[0]*2]\n",
    "        temp_list[b[1]*2+1]=alpha_pose[b[0]*2+1]\n",
    "    human36m=np.asarray(temp_list)\n",
    "    return human36m\n",
    "\n",
    "#This function is includding nose for classification\n",
    "def map_alpha_to_human_classification(alpha_pose):\n",
    "    alpha_pose=add_features(alpha_pose)\n",
    "    temp_list = [None] * 64\n",
    "    for a,b in index_mapping_nose.items():\n",
    "        temp_list[b[1]*2]=alpha_pose[b[0]*2]\n",
    "        temp_list[b[1]*2+1]=alpha_pose[b[0]*2+1]\n",
    "    human36m=np.asarray(temp_list)\n",
    "    return human36m\n",
    "\n",
    "\n",
    "def map_alpha_to_human_classification_json(path):\n",
    "    # Opening JSON file \n",
    "    f = open(path) \n",
    "    converted=[]\n",
    "    # returns JSON object as  \n",
    "    # a dictionary \n",
    "    data = json.load(f) \n",
    "\n",
    "    for dat in data:\n",
    "        convert=np.asarray(data_converter(dat))\n",
    "        human36m_alpha_example=map_alpha_to_human_classification(convert)\n",
    "        human36m_alpha_example=human36m_alpha_example.astype('float')\n",
    "        converted.append(human36m_alpha_example)\n",
    "\n",
    "    converted=np.asarray(converted) \n",
    "        \n",
    "    # Closing file \n",
    "    f.close() \n",
    "    \n",
    "    return converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do 3D Prediction from Custom Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Created Statistic Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize=True\n",
    "actions = data_utils.define_actions( FLAGS.action )\n",
    "# Human3.6m IDs for training and testing\n",
    "TRAIN_SUBJECTS = [1,5,6,7,8]\n",
    "TEST_SUBJECTS  = [9,11]\n",
    "\n",
    "\n",
    "stat_3D = torch.load('../data/stat_3d.pth.tar')\n",
    "stat_2D = torch.load('../data/stat_2d.pth.tar')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_mean_2d, data_std_2d, dim_to_ignore_2d, dim_to_use_2d = stat_2D['mean'],stat_2D['std'],stat_2D['dim_ignore'],stat_2D['dim_use']\n",
    "data_mean_3d, data_std_3d, dim_to_ignore_3d, dim_to_use_3d = stat_3D['mean'],stat_3D['std'],stat_3D['dim_ignore'],stat_3D['dim_use']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the json data\n",
    "#load json data using json,load(f)\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "class Human36M_testing(Dataset):\n",
    "    def __init__(self, skeleton,many=False):\n",
    "        \"\"\"\n",
    "        :param actions: list of actions to use\n",
    "        :param data_path: path to dataset\n",
    "        :param use_hg: use stacked hourglass detections\n",
    "        :param is_train: load train/test dataset\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_inp, self.test_out = [], []\n",
    "\n",
    "        # loading data\n",
    "        # load test data\n",
    "       \n",
    "        if many:\n",
    "            num_f= skeleton.shape\n",
    "            for i in range(num_f[0]):\n",
    "                self.test_inp.append(skeleton[i])\n",
    "        else:\n",
    "            self.test_inp.append(skeleton)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs = torch.from_numpy(self.test_inp[index]).float()\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_inp)\n",
    "    \n",
    "def normalize_single_data(data, data_mean, data_std, dim_to_use ):\n",
    "    \"\"\"Normalizes a dictionary of poses\n",
    "\n",
    "    Args\n",
    "    data: dictionary where values are\n",
    "    data_mean: np vector with the mean of the data\n",
    "    data_std: np vector with the standard deviation of the data\n",
    "    dim_to_use: list of dimensions to keep in the data\n",
    "    Returns\n",
    "    data_out: dictionary with same keys as data, but values have been normalized\n",
    "    \"\"\"\n",
    "\n",
    "    data= data[dim_to_use]\n",
    "    mu = data_mean[dim_to_use]\n",
    "    stddev = data_std[dim_to_use]\n",
    "    data_out= np.divide( (data - mu), stddev )\n",
    "\n",
    "    return data_out\n",
    "def create_datatest(data):\n",
    "    converted=[]\n",
    "    for dat in data:\n",
    "        convert=np.asarray(data_converter(dat))\n",
    "        human36m_alpha_example=map_alpha_to_human(convert)\n",
    "        normalized=normalize_single_data(human36m_alpha_example,data_mean_2d,data_std_2d,dim_to_use_2d)\n",
    "        normalized=normalized.astype('float')\n",
    "        converted.append(normalized)\n",
    "\n",
    "    converted=np.asarray(converted) \n",
    "    test_loader = DataLoader(\n",
    "        dataset=Human36M_testing(converted,True),\n",
    "        batch_size=1024,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> creating model\n",
      ">>> total params: 10.60M\n",
      ">>> loading ckpt from '../../3d_pose_baseline_pytorch/checkpoint/test/ckpt_best.pth.tar'\n",
      ">>> ckpt loaded (epoch: 29 | err: 40.643307541837174)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Project Lab\\THESIS_FALL_DETECTION\\2D to 3D Pose\\3d_pose_baseline_pytorch\\src\\model.py:11: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight)\n",
      "C:\\Users\\hutom\\anaconda3\\envs\\3DPose\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_path='../checkpoint/optuna/ckpt_best_1.pth.tar'\n",
    "# create model\n",
    "print(\">>> creating model\")\n",
    "#model = LinearModel()\n",
    "model = OptunaModel()\n",
    "model = model.cuda()\n",
    "model.apply(weight_init)\n",
    "print(\">>> total params: {:.2f}M\".format(sum(p.numel() for p in model.parameters()) / 1000000.0))\n",
    "criterion = nn.MSELoss(size_average=True).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3)\n",
    "\n",
    "print(\">>> loading ckpt from '{}'\".format('../../3d_pose_baseline_pytorch/checkpoint/test/ckpt_best.pth.tar'))\n",
    "ckpt = torch.load(model_path)\n",
    "start_epoch = ckpt['epoch']\n",
    "err_best = ckpt['err']\n",
    "glob_step = ckpt['step']\n",
    "lr_now = ckpt['lr']\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "optimizer.load_state_dict(ckpt['optimizer'])\n",
    "print(\">>> ckpt loaded (epoch: {} | err: {})\".format(start_epoch, err_best))\n",
    "\n",
    "new_stat_3d={}\n",
    "new_stat_3d['mean']=data_mean_3d\n",
    "new_stat_3d['std']=data_std_3d\n",
    "new_stat_3d['dim_use']=dim_to_use_3d\n",
    "new_stat_3d['dim_ignore']=dim_to_ignore_3d\n",
    "    \n",
    "def test(test_loader, model, criterion, stat_3d, procrustes=False):\n",
    "    losses = utils.AverageMeter()\n",
    "    model.eval()\n",
    "\n",
    "    all_dist = []\n",
    "    pred_result=[]\n",
    "    start = time.time()\n",
    "    batch_time = 0\n",
    "    bar = Bar('>>>', fill='>', max=len(test_loader))\n",
    "\n",
    "    for i, inps in enumerate(test_loader):\n",
    "        inputs = Variable(inps.cuda())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        # calculate erruracy\n",
    "        print(outputs.shape)\n",
    "        outputs_unnorm = data_process.unNormalizeData(outputs.data.cpu().numpy(), stat_3d['mean'], stat_3d['std'], stat_3d['dim_use'])\n",
    "\n",
    "        # remove dim ignored\n",
    "        dim_use = np.hstack((np.arange(3), stat_3d['dim_use']))\n",
    "\n",
    "        outputs_use = outputs_unnorm[:, dim_use]\n",
    "        pred_result.append(outputs_unnorm)\n",
    "        \n",
    "        # update summary\n",
    "        if (i + 1) % 100 == 0:\n",
    "            batch_time = time.time() - start\n",
    "            start = time.time()\n",
    "\n",
    "        bar.suffix = '({batch}/{size}) | batch: {batchtime:.4}ms | Total: {ttl} | ETA: {eta:} | loss: {loss:.6f}' \\\n",
    "            .format(batch=i + 1,\n",
    "                    size=len(test_loader),\n",
    "                    batchtime=batch_time * 10.0,\n",
    "                    ttl=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg)\n",
    "        bar.next()\n",
    "    bar.finish()\n",
    "    return pred_result\n",
    "\n",
    "# Combine prediction from each batch into one prediction\n",
    "def combine_prediction(pred_result_all):\n",
    "    prediction_list = []\n",
    "    for pred in pred_result_all:\n",
    "        for pre in pred:\n",
    "            prediction_list.append(pre)\n",
    "    prediction_list=np.asarray(prediction_list)\n",
    "    return prediction_list\n",
    "\n",
    "\n",
    "def correct_3D(poses3d_input,poses2d_normalized):\n",
    "    _max = 0\n",
    "    _min = 10000\n",
    "    poses3d=np.copy(poses3d_input)\n",
    "    \n",
    "    spine_x = poses2d_normalized[0][24]\n",
    "    spine_y = poses2d_normalized[0][25]\n",
    "            \n",
    "    \n",
    "    for i in range(poses3d.shape[0]):\n",
    "\n",
    "        for j in range(32):\n",
    "\n",
    "            tmp = poses3d[i][j * 3 + 2]\n",
    "            poses3d[i][j * 3 + 2] = poses3d[i][j * 3 + 1]\n",
    "            poses3d[i][j * 3 + 1] = tmp\n",
    "            if poses3d[i][j * 3 + 2] > _max:\n",
    "                _max = poses3d[i][j * 3 + 2]\n",
    "                print(\"_max: \",_max)\n",
    "            if poses3d[i][j * 3 + 2] < _min:\n",
    "                _min = poses3d[i][j * 3 + 2]\n",
    "                print(\"_min: \",_min)\n",
    "\n",
    "    for i in range(poses3d.shape[0]):\n",
    "        for j in range(32):\n",
    "            poses3d[i][j * 3 + 2] = _max - poses3d[i][j * 3 + 2] + _min\n",
    "            poses3d[i][j * 3] += (spine_x - 630)\n",
    "            poses3d[i][j * 3 + 2] += (500 - spine_y)\n",
    "\n",
    "    return poses3d\n",
    "\n",
    "def inferencealphaposeto3D(path,fixing=False,save_npy=False):\n",
    "    # Opening JSON file \n",
    "    f = open(path) \n",
    "\n",
    "    # returns JSON object as  \n",
    "    # a dictionary \n",
    "    data = json.load(f) \n",
    "\n",
    "\n",
    "    # Closing file \n",
    "    f.close() \n",
    "    \n",
    "    #Create Datatest for 2D to 3D Inference\n",
    "    all_test_data=create_datatest(data)\n",
    "    \n",
    "    #Doing Inference\n",
    "    pred_result_all=test(all_test_data, model, criterion, new_stat_3d) #All\n",
    "    \n",
    "    #Combine Prediction Result\n",
    "    prediction_list=combine_prediction(pred_result_all)\n",
    "    \n",
    "    if fixing:\n",
    "        #Fixing for unity\n",
    "        test_2d_normalized = np.asarray(all_test_data.dataset.test_inp) \n",
    "        fixed=correct_3D(prediction_list,test_2d_normalized)\n",
    "    else:\n",
    "        fixed=prediction_list\n",
    "    \n",
    "    if save_npy:\n",
    "        base=os.path.basename(path)\n",
    "        base=os.path.splitext(base)[0]\n",
    "\n",
    "        with open('../inference_result_npy/'+base+'.npy', 'wb') as f:\n",
    "            np.save(f, fixed)\n",
    "    \n",
    "    return fixed\n",
    "\n",
    "\n",
    "\n",
    "def save_to_json(result_3D, input_path, output_path):\n",
    "    f = open(input_path) \n",
    "    dim_use = np.hstack((np.arange(3), dim_to_use_3d))\n",
    "\n",
    "    # returns JSON object as  \n",
    "    # a dictionary \n",
    "    data = json.load(f) \n",
    "    \n",
    "    for a,b in zip(data,result_3D):\n",
    "        a['keypoints']=b[dim_use].tolist()\n",
    "        a['visualize']=b.tolist()\n",
    "        \n",
    "    with open(output_path, 'w') as fp:\n",
    "        fp.write(json.dumps(data))\n",
    "    \n",
    "def save_to_json_2D(result_2D, input_path, output_path):\n",
    "    f = open(input_path) \n",
    "\n",
    "    # returns JSON object as  \n",
    "    # a dictionary \n",
    "    data = json.load(f) \n",
    "    \n",
    "    for a,b in zip(data,result_2D):\n",
    "        a['keypoints']=b[dim_to_use_2d].tolist()\n",
    "        \n",
    "    with open(output_path, 'w') as fp:\n",
    "        fp.write(json.dumps(data))\n",
    "        \n",
    "def save_to_json_original2D(result_2D, input_path, output_path):\n",
    "    f = open(input_path) \n",
    "\n",
    "    # returns JSON object as  \n",
    "    # a dictionary \n",
    "    data = json.load(f) \n",
    "    \n",
    "    a=dim_to_use_2d.tolist()\n",
    "    a.insert(18,28)\n",
    "    a.insert(19,29)\n",
    "    dim_to_use_2d_nose=np.asarray(a)\n",
    "\n",
    "    for a,b in zip(data,result_2D):\n",
    "        a['keypoints']=b[dim_to_use_2d_nose].tolist()\n",
    "        \n",
    "    with open(output_path, 'w') as fp:\n",
    "        fp.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference to json to three kinds:\n",
    "1. Original 3D Baseline\n",
    "2. 3D After doing proscrutes to another angle\n",
    "3. 2D Mapping after doing proscrutes to another angle\n",
    "4. Eucledian normalization of 2D Mapping after doing proscrutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original 3D Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([1024, 48])\n",
      "torch.Size([967, 48])\n",
      "Wall time: 766 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path='../json_test/taoyuan_angle2.json'\n",
    "base= os.path.splitext(os.path.basename(path))[0]\n",
    "inference_result=inferencealphaposeto3D(path,fixing=False,save_npy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='../inference_result_npy/'+base+'_3D_Original.json'\n",
    "save_to_json(inference_result, path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original 2D Human3.6m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will save alpha pose in human3.6m format that have nose inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='../inference_result_npy/'+base+'_2D_Original.json'\n",
    "mapping_result=map_alpha_to_human_classification_json(path)\n",
    "save_to_json_original2D(mapping_result, path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D After doing proscrutes to another angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_transformation_3D(skeleton_a,skeleton_b,fullskeleton):\n",
    "    \n",
    "    # remove dim ignored\n",
    "    dim_use = np.hstack((np.arange(3), new_stat_3d['dim_use']))\n",
    "\n",
    "    gt = skeleton_a[dim_use]\n",
    "    out = skeleton_b[dim_use]\n",
    "    gt = gt.reshape(-1, 3)\n",
    "    out = out.reshape(-1, 3)\n",
    "    _, Z, T, b, c = get_transformation(gt, out, True)\n",
    "    transformation_value={\"T\":T,\"b\":b,\"c\":c}\n",
    "    \n",
    "    skeleton3D=[]\n",
    "    for i,skeleton in enumerate(fullskeleton):\n",
    "        skeleton = skeleton.reshape(-1, 3)\n",
    "        skeleton = (b * skeleton.dot(T)) + c\n",
    "        skeleton3D.append(skeleton)\n",
    "    skeleton3D=np.asarray(skeleton3D)\n",
    "    skeleton3D=skeleton3D.reshape(skeleton3D.shape[0],96)\n",
    "\n",
    "    return  skeleton3D,transformation_value\n",
    "    \n",
    "def find_pair_transformation(path_1,pose_class_1,image_id_1,path_2,pose_class_2,image_id_2):\n",
    "    with open(path_1, 'rb') as f:\n",
    "        angle1 = json.load(f)\n",
    "    with open(path_2, 'rb') as f:\n",
    "        angle2 = json.load(f)\n",
    "    \n",
    "    angle1_temp=[]\n",
    "    angle1_all=[]\n",
    "\n",
    "    for a in angle1:\n",
    "        if (a['pose_class']==pose_class_1) and (a['image_id']==image_id_1):\n",
    "            angle1_temp.append(a)\n",
    "    \n",
    "\n",
    "    angle2_before=[]\n",
    "    angle2_all=[]\n",
    "    for a in angle2:\n",
    "        if (a['pose_class']==pose_class_2) and (a['image_id']==image_id_2):\n",
    "            angle2_before.append(a)\n",
    "        angle2_all.append(a['visualize'])\n",
    "    \n",
    "    angle2_transformed_3D,transformation_value=find_transformation_3D(np.asarray(angle1_temp[0]['visualize']),\n",
    "                                             np.asarray(angle2_before[0]['visualize']),\n",
    "                                             np.asarray(angle2_all))\n",
    "    \n",
    "\n",
    "    return angle2_transformed_3D,transformation_value\n",
    "\n",
    "def get_transformation(X, Y, compute_optimal_scale=True):\n",
    "    muX = X.mean(0)\n",
    "    muY = Y.mean(0)\n",
    "\n",
    "    X0 = X - muX\n",
    "    Y0 = Y - muY\n",
    "\n",
    "    ssX = (X0 ** 2.).sum()\n",
    "    ssY = (Y0 ** 2.).sum()\n",
    "\n",
    "    # centred Frobenius norm\n",
    "    normX = np.sqrt(ssX)\n",
    "    normY = np.sqrt(ssY)\n",
    "\n",
    "    # scale to equal (unit) norm\n",
    "    X0 = X0 / normX\n",
    "    Y0 = Y0 / normY\n",
    "\n",
    "    # optimum rotation matrix of Y\n",
    "    A = np.dot(X0.T, Y0)\n",
    "    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "    V = Vt.T\n",
    "    T = np.dot(V, U.T)\n",
    "\n",
    "    # Make sure we have a rotation\n",
    "    detT = np.linalg.det(T)\n",
    "    V[:, -1] *= np.sign(detT)\n",
    "    s[-1] *= np.sign(detT)\n",
    "    T = np.dot(V, U.T)\n",
    "\n",
    "    traceTA = s.sum()\n",
    "\n",
    "    if compute_optimal_scale:  # Compute optimum scaling of Y.\n",
    "        b = traceTA * normX / normY\n",
    "        d = 1 - traceTA ** 2\n",
    "        Z = normX * traceTA * np.dot(Y0, T) + muX\n",
    "    else:  # If no scaling allowed\n",
    "        b = 1\n",
    "        d = 1 + ssY / ssX - 2 * traceTA * normY / normX\n",
    "        Z = normY * np.dot(Y0, T) + muX\n",
    "\n",
    "    c = muX - b * np.dot(muY, T)\n",
    "\n",
    "    return d, Z, T, b, c\n",
    "\n",
    "def save_transformation(transformed_value,output_path,path_1,pose_class_1,image_id_1,path_2,pose_class_2,image_id_2):\n",
    "    base_1= os.path.splitext(os.path.basename(path_1))[0]\n",
    "    base_2= os.path.splitext(os.path.basename(path_2))[0]\n",
    "    transformed_value['Reference_Path']=base_1\n",
    "    transformed_value['Reference_Pose_Class']=pose_class_1\n",
    "    transformed_value['Reference_Image_ID']=image_id_1\n",
    "    transformed_value['Transformed_Path']=base_2\n",
    "    transformed_value['Transformed_Pose_Class']=pose_class_2\n",
    "    transformed_value['Transformed_Image_ID']=image_id_2\n",
    "    \n",
    "    \n",
    "\n",
    "    try:\n",
    "        import cPickle as pickle\n",
    "    except ImportError:  # Python 3.x\n",
    "        import pickle\n",
    "        \n",
    "    with open(output_path, 'wb') as fp:\n",
    "        pickle.dump(transformed_value, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Reference\n",
    "path_1='../inference_result_npy/taoyuan_angle1_3D_Original.json'\n",
    "pose_class_1=\"Standing_01\"\n",
    "image_id_1= '00631.png'\n",
    "\n",
    "#Want to transform\n",
    "path_2='../inference_result_npy/taoyuan_angle2_3D_Original.json'\n",
    "pose_class_2=\"Standing_02\"\n",
    "image_id_2= '00646.png'\n",
    "\n",
    "base_1= os.path.splitext(os.path.basename(path_1))[0]\n",
    "base_2= os.path.splitext(os.path.basename(path_2))[0]\n",
    "\n",
    "\n",
    "transformed,transformation_value=find_pair_transformation(path_1,pose_class_1,image_id_1,path_2,pose_class_2,image_id_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='../inference_result_npy/'+base_2+\" to \"+base_1+'_proscrustes.json'\n",
    "save_to_json(transformed, path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='../inference_result_npy/'+base_2+\" to \"+base_1+'_transformationvalue.pickle'\n",
    "save_transformation(transformation_value, output_path,path_1,pose_class_1,image_id_1,path_2,pose_class_2,image_id_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Mapping after doing proscrutes to another angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def load_cameras(bpath='cameras.h5', subjects=None):\n",
    "    \"\"\"\n",
    "    :param bpath: *.h5\n",
    "    :param subjects:\n",
    "    :return: (dict)\n",
    "    \"\"\"\n",
    "\n",
    "    if subjects is None:\n",
    "        subjects = [1, 5, 6, 7, 8, 9, 11]\n",
    "    rcams = {}\n",
    "\n",
    "    with h5py.File(bpath, 'r') as hf:\n",
    "        for s in subjects:\n",
    "            for c in range(4):  # There are 4 cameras in human3.6m\n",
    "                a = load_camera_params(hf, 'subject%d/camera%d/{0}' % (s, c + 1))\n",
    "                rcams[(s, c + 1)] = a\n",
    "\n",
    "    return rcams\n",
    "\n",
    "\n",
    "def map3dto2dcamera( poses_set, cams, ncams=4 ):\n",
    "    \"\"\"\n",
    "    Project 3d poses using camera parameters\n",
    "\n",
    "    cams: dictionary with camera parameters\n",
    "    ncams: number of cameras per subject\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for cam in range( ncams ):\n",
    "        R, T, f, c, k, p, name = cams[ (11, cam+1) ]\n",
    "        pts2d, _, _, _, _ = project_point_radial( np.reshape(poses_set, [-1, 3]), R, T, f, c, k, p )\n",
    "\n",
    "        pts2d = np.reshape( pts2d, [-1, len(H36M_NAMES)*2] )\n",
    "\n",
    "    return pts2d\n",
    "\n",
    "def project_point_radial(P, R, T, f, c, k, p):\n",
    "    \"\"\"\n",
    "    Args\n",
    "    P: Nx3 points in world coordinates\n",
    "    R: 3x3 Camera rotation matrix\n",
    "    T: 3x1 Camera translation parameters\n",
    "    f: 2x1 (scalar) Camera focal length\n",
    "    c: 2x1 Camera center\n",
    "    k: 3x1 Camera radial distortion coefficients\n",
    "    p: 2x1 Camera tangential distortion coefficients\n",
    "    Returns\n",
    "    Proj: Nx2 points in pixel space\n",
    "    D: 1xN depth of each point in camera space\n",
    "    radial: 1xN radial distortion per point\n",
    "    tan: 1xN tangential distortion per point\n",
    "    r2: 1xN squared radius of the projected points before distortion\n",
    "    \"\"\"\n",
    "\n",
    "    # P is a matrix of 3-dimensional points\n",
    "    assert len(P.shape) == 2\n",
    "    assert P.shape[1] == 3\n",
    "\n",
    "    N = P.shape[0]\n",
    "    X = R.dot(P.T - T)  # rotate and translate\n",
    "    XX = X[:2, :] / X[2, :]  # 2x16\n",
    "    r2 = XX[0, :] ** 2 + XX[1, :] ** 2  # 16,\n",
    "\n",
    "    radial = 1 + np.einsum('ij,ij->j', np.tile(k, (1, N)), np.array([r2, r2 ** 2, r2 ** 3]))  # 16,\n",
    "    tan = p[0] * XX[1, :] + p[1] * XX[0, :]  # 16,\n",
    "\n",
    "    tm = np.outer(np.array([p[1], p[0]]).reshape(-1), r2)  # 2x16\n",
    "\n",
    "    XXX = XX * np.tile(radial + tan, (2, 1)) + tm  # 2x16\n",
    "\n",
    "    Proj = (f * XXX) + c  # 2x16\n",
    "    Proj = Proj.T\n",
    "\n",
    "    D = X[2, ]\n",
    "\n",
    "    return Proj, D, radial, tan, r2\n",
    "\n",
    "\n",
    "def mapto2D(skeletons,rcams):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    skeleton2D=[]\n",
    "    for i,skeleton in enumerate(skeletons):\n",
    "        mapped=map3dto2dcamera(skeleton,rcams,4)\n",
    "        skeleton2D.append(mapped)\n",
    "    skeleton2D=np.asarray(skeleton2D)\n",
    "    skeleton2D=skeleton2D.reshape(skeleton2D.shape[0],64)\n",
    "    return skeleton2D\n",
    "\n",
    "SUBJECT_IDS = [1,5,6,7,8,9,11]\n",
    "this_file = os.path.dirname(os.path.realpath('__file__'))\n",
    "rcams = cameras.load_cameras(os.path.join(this_file, \"../data/h36m/metadata.xml\"), SUBJECT_IDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "skeleton2d=mapto2D(transformed,rcams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='../inference_result_npy/'+base+'_2D_Proscrutes.json'\n",
    "save_to_json_2D(skeleton2d, path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize using Eucledian Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_human36m={\n",
    "    # Use 16 skeleton point\n",
    "    \"Pelvis\": 0,\n",
    "    \"Head\": 15,\n",
    "    \"Thorax\":13,\n",
    "    \"RShoulder\": 25,\n",
    "    \"RElbow\": 26,\n",
    "    \"RWrist\": 27,\n",
    "    \"LShoulder\": 17,\n",
    "    \"LElbow\": 18,\n",
    "    \"LWrist\": 19,\n",
    "    \"RHip\": 1,\n",
    "    \"RKnee\": 2,\n",
    "    \"RAnkle\": 3,\n",
    "    \"LHip\": 6,\n",
    "    \"LKnee\": 7,\n",
    "    \"LAnkle\": 8,\n",
    "    \"Spine\" : 12\n",
    "}\n",
    "\n",
    "def euclidean_dist(a, b):\n",
    "    # This function calculates the euclidean distance between 2 point in 2-D coordinates\n",
    "    # if one of two points is (0,0), dist = 0\n",
    "    # a, b: input array with dimension: m, 2\n",
    "    # m: number of samples\n",
    "    # 2: x and y coordinate\n",
    "    try:\n",
    "        if (a.shape[1] == 2 and a.shape == b.shape):\n",
    "            # check if element of a and b is (0,0)\n",
    "            bol_a = (a[:,0] != 0).astype(int)\n",
    "            bol_b = (b[:,0] != 0).astype(int)\n",
    "            dist = np.linalg.norm(a-b, axis=1)\n",
    "            return((dist*bol_a*bol_b).reshape(a.shape[0],1))\n",
    "    except:\n",
    "        print(\"[Error]: Check dimension of input vector\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def norm_human36m(X):\n",
    "    num_sample = X.shape[0]\n",
    "    # Keypoints\n",
    "    Pelvis = X[:,index_human36m['Pelvis']*2:index_human36m['Pelvis']*2+2]\n",
    "    Head = X[:,index_human36m['Head']*2:index_human36m['Head']*2+2]\n",
    "    Thorax = X[:,index_human36m['Thorax']*2:index_human36m['Thorax']*2+2]\n",
    "    RShoulder = X[:,index_human36m['RShoulder']*2:index_human36m['RShoulder']*2+2]\n",
    "    RElbow = X[:,index_human36m['RElbow']*2:index_human36m['RElbow']*2+2]\n",
    "    RWrist = X[:,index_human36m['RWrist']*2:index_human36m['RWrist']*2+2]\n",
    "    LShoulder = X[:,index_human36m['LShoulder']*2:index_human36m['LShoulder']*2+2]\n",
    "    LElbow = X[:,index_human36m['LElbow']*2:index_human36m['LElbow']*2+2]\n",
    "    LWrist = X[:,index_human36m['LWrist']*2:index_human36m['LWrist']*2+2]\n",
    "    RHip = X[:,index_human36m['RHip']*2:index_human36m['RHip']*2+2]\n",
    "    RKnee = X[:,index_human36m['RKnee']*2:index_human36m['RKnee']*2+2]\n",
    "    RAnkle = X[:,index_human36m['RAnkle']*2:index_human36m['RAnkle']*2+2]\n",
    "    LHip = X[:,index_human36m['LHip']*2:index_human36m['LHip']*2+2]\n",
    "    LKnee = X[:,index_human36m['LKnee']*2:index_human36m['LKnee']*2+2]\n",
    "    LAnkle = X[:,index_human36m['LAnkle']*2:index_human36m['LAnkle']*2+2]\n",
    "\n",
    "\n",
    "    # Length of head\n",
    "    length_Head_Thorax = euclidean_dist(Head, Thorax)\n",
    "    length_head      = np.maximum.reduce([length_Head_Thorax])\n",
    "\n",
    "    # Length of torso\n",
    "    length_Thorax_LHip = euclidean_dist(Thorax, LHip)\n",
    "    length_Thorax_RHip = euclidean_dist(Thorax, RHip)\n",
    "    length_torso     = np.maximum(length_Thorax_LHip, length_Thorax_RHip)\n",
    "\n",
    "    # Length of right leg\n",
    "    length_leg_right = euclidean_dist(RHip, RKnee) + euclidean_dist(RKnee, RAnkle)\n",
    "  \n",
    "    # Length of left leg\n",
    "    length_leg_left = euclidean_dist(LHip, LKnee) + euclidean_dist(LKnee, LAnkle)\n",
    "\n",
    "\n",
    "    # Length of leg\n",
    "    length_leg = np.maximum(length_leg_right, length_leg_left)\n",
    "\n",
    "    # Length of body\n",
    "    length_body = length_head + length_torso + length_leg\n",
    "    \n",
    "    # Check all samples have length_body of 0\n",
    "    length_chk = (length_body > 0).astype(int)\n",
    "    \n",
    "    # Check keypoints at origin\n",
    "    keypoints_chk = (X > 0).astype(int)\n",
    "    \n",
    "    chk = length_chk * keypoints_chk\n",
    "    \n",
    "    # Set all length_body of 0 to 1 (to avoid division by 0)\n",
    "    length_body[length_body == 0] = 1\n",
    "    \n",
    "    # The center of gravity\n",
    "    num_pts = (X[:, 0::2] > 0).sum(1).reshape(num_sample,1)\n",
    "\n",
    "    centr_x = X[:, 0::2].sum(1).reshape(num_sample,1) / num_pts\n",
    "    centr_y = X[:, 1::2].sum(1).reshape(num_sample,1) / num_pts\n",
    "\n",
    "    # The  coordinates  are  normalized relative to the length of the body and the center of gravity\n",
    "    X_norm_x = (X[:, 0::2] - centr_x) / length_body\n",
    "    X_norm_y = (X[:, 1::2] - centr_y) / length_body\n",
    "    \n",
    "    # Stack 1st element x and y together\n",
    "    X_norm = np.column_stack((X_norm_x[:,:1], X_norm_y[:,:1]))\n",
    "        \n",
    "    for i in range(1, X.shape[1]//2):\n",
    "        X_norm = np.column_stack((X_norm, X_norm_x[:,i:i+1], X_norm_y[:,i:i+1]))\n",
    "    \n",
    "    # Set all samples have length_body of 0 to origin (0, 0)\n",
    "    X_norm = X_norm * chk\n",
    "    \n",
    "    return X_norm\n",
    "\n",
    "def euclidean_dist_3D(a, b):\n",
    "\n",
    "    try:\n",
    "        if (a.shape[1] == 3 and a.shape == b.shape):\n",
    "            # check if element of a and b is (0,0)\n",
    "            bol_a = (a[:,0] != 0).astype(int)\n",
    "            bol_b = (b[:,0] != 0).astype(int)\n",
    "            dist = np.linalg.norm(a-b, axis=1)\n",
    "            return((dist*bol_a*bol_b).reshape(a.shape[0],1))\n",
    "    except:\n",
    "        print(\"[Error]: Check dimension of input vector\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def norm_human36m_3D(X):\n",
    "    num_sample = X.shape[0]\n",
    "    # Keypoints\n",
    "    Pelvis = X[:,index_human36m['Pelvis']*3:index_human36m['Pelvis']*3+3]\n",
    "    Head = X[:,index_human36m['Head']*3:index_human36m['Head']*3+3]\n",
    "    Thorax = X[:,index_human36m['Thorax']*3:index_human36m['Thorax']*3+3]\n",
    "    RShoulder = X[:,index_human36m['RShoulder']*3:index_human36m['RShoulder']*3+3]\n",
    "    RElbow = X[:,index_human36m['RElbow']*3:index_human36m['RElbow']*3+3]\n",
    "    RWrist = X[:,index_human36m['RWrist']*3:index_human36m['RWrist']*3+3]\n",
    "    LShoulder = X[:,index_human36m['LShoulder']*3:index_human36m['LShoulder']*3+3]\n",
    "    LElbow = X[:,index_human36m['LElbow']*3:index_human36m['LElbow']*3+3]\n",
    "    LWrist = X[:,index_human36m['LWrist']*3:index_human36m['LWrist']*3+3]\n",
    "    RHip = X[:,index_human36m['RHip']*3:index_human36m['RHip']*3+3]\n",
    "    RKnee = X[:,index_human36m['RKnee']*3:index_human36m['RKnee']*3+3]\n",
    "    RAnkle = X[:,index_human36m['RAnkle']*3:index_human36m['RAnkle']*3+3]\n",
    "    LHip = X[:,index_human36m['LHip']*3:index_human36m['LHip']*3+3]\n",
    "    LKnee = X[:,index_human36m['LKnee']*3:index_human36m['LKnee']*3+3]\n",
    "    LAnkle = X[:,index_human36m['LAnkle']*3:index_human36m['LAnkle']*3+3]\n",
    "\n",
    "\n",
    "    # Length of head\n",
    "    length_Head_Thorax = euclidean_dist_3D(Head, Thorax)\n",
    "    length_head      = np.maximum.reduce([length_Head_Thorax])\n",
    "\n",
    "    # Length of torso\n",
    "    length_Thorax_LHip = euclidean_dist_3D(Thorax, LHip)\n",
    "    length_Thorax_RHip = euclidean_dist_3D(Thorax, RHip)\n",
    "    length_torso     = np.maximum(length_Thorax_LHip, length_Thorax_RHip)\n",
    "\n",
    "    # Length of right leg\n",
    "    length_leg_right = euclidean_dist_3D(RHip, RKnee) + euclidean_dist_3D(RKnee, RAnkle)\n",
    "  \n",
    "    # Length of left leg\n",
    "    length_leg_left = euclidean_dist_3D(LHip, LKnee) + euclidean_dist_3D(LKnee, LAnkle)\n",
    "\n",
    "\n",
    "    # Length of leg\n",
    "    length_leg = np.maximum(length_leg_right, length_leg_left)\n",
    "\n",
    "    # Length of body\n",
    "    length_body = length_head + length_torso + length_leg\n",
    "    \n",
    "    # Check all samples have length_body of 0\n",
    "    length_chk = (length_body > 0).astype(int)\n",
    "    \n",
    "    # Check keypoints at origin\n",
    "    keypoints_chk = (X > 0).astype(int)\n",
    "    \n",
    "    chk = length_chk * keypoints_chk\n",
    "    \n",
    "    # Set all length_body of 0 to 1 (to avoid division by 0)\n",
    "    length_body[length_body == 0] = 1\n",
    "    \n",
    "    # The center of gravity\n",
    "    num_pts = np.full((num_sample, 1), 32)\n",
    "    print(num_pts)\n",
    "    centr_x = X[:, 0::3].sum(1).reshape(num_sample,1) / num_pts\n",
    "    centr_y = X[:, 1::3].sum(1).reshape(num_sample,1) / num_pts\n",
    "    centr_z = X[:, 2::3].sum(1).reshape(num_sample,1) / num_pts\n",
    "\n",
    "    # The  coordinates  are  normalized relative to the length of the body and the center of gravity\n",
    "    X_norm_x = (X[:, 0::3] - centr_x) / length_body\n",
    "    X_norm_y = (X[:, 1::3] - centr_y) / length_body\n",
    "    X_norm_z = (X[:, 2::3] - centr_z) / length_body\n",
    "\n",
    "    # Stack 1st element x and y together\n",
    "    X_norm = np.column_stack((X_norm_x[:,:1], X_norm_y[:,:1], X_norm_z[:,:1]))\n",
    "        \n",
    "    for i in range(1, X.shape[1]//3):\n",
    "        X_norm = np.column_stack((X_norm, X_norm_x[:,i:i+1], X_norm_y[:,i:i+1], X_norm_z[:,i:i+1]))\n",
    "    \n",
    "    # Set all samples have length_body of 0 to origin (0, 0)\n",
    "    X_norm = X_norm * chk\n",
    "    \n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "normalized_eucledianhuman36m=norm_human36m(skeleton2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_eucledianhuman36m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='../inference_result_npy/'+base+'_2D_Proscrutes_Normalized.json'\n",
    "save_to_json_2D(normalized_eucledianhuman36m, path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Code (Convert only 1 Skeleton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferencealphaposeto3D_one(alpha2d,input_type=\"json\"):\n",
    "    converted=[]\n",
    "    human2d=alpha2d.copy()\n",
    "    human3d=alpha2d.copy()\n",
    "    \n",
    "    if (input_type==\"json\"):\n",
    "        convert=np.asarray(data_converter(alpha2d))\n",
    "    else:\n",
    "        convert=alpha2d\n",
    "        \n",
    "    a=dim_to_use_2d.tolist()\n",
    "    a.insert(18,28)\n",
    "    a.insert(19,29)\n",
    "    dim_to_use_2d_nose=np.asarray(a)\n",
    "\n",
    "    human36m_output=map_alpha_to_human_classification(convert)\n",
    "    human36m_output=human36m_output.astype('float')\n",
    "    human2d['keypoints']=human36m_output[dim_to_use_2d_nose].tolist()\n",
    "\n",
    "    human36m_alpha_example=map_alpha_to_human(convert)\n",
    "    normalized=normalize_single_data(human36m_alpha_example,data_mean_2d,data_std_2d,dim_to_use_2d)\n",
    "    normalized=normalized.astype('float')\n",
    "    converted.append(normalized)\n",
    "    converted=np.asarray(converted) \n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        dataset=Human36M_testing(converted,True),\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True)\n",
    "\n",
    "    #Doing Inference\n",
    "    pred_result_all=test(test_loader, model, criterion, new_stat_3d) #All\n",
    "    dim_use = np.hstack((np.arange(3), dim_to_use_3d))\n",
    "    prediction=pred_result_all[0][0][dim_use]\n",
    "    human3d['keypoints']=prediction.tolist()\n",
    "        \n",
    "    return human3d,human2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../json_test/taoyuan_angle2.json'\n",
    "f = open(path) \n",
    "data = json.load(f) \n",
    "# input_to=np.asarray(data_converter(data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': '00001.png',\n",
       " 'category_id': 1,\n",
       " 'pose_class': 'FallingDown_02',\n",
       " 'keypoints': [1498.716064453125,\n",
       "  483.880615234375,\n",
       "  0.9190043807029724,\n",
       "  1518.553466796875,\n",
       "  470.65570068359375,\n",
       "  0.9054306149482727,\n",
       "  1478.878662109375,\n",
       "  470.65570068359375,\n",
       "  0.8983249068260193,\n",
       "  1551.61572265625,\n",
       "  437.5933837890625,\n",
       "  0.7453621029853821,\n",
       "  1445.81640625,\n",
       "  437.5933837890625,\n",
       "  0.7723419070243835,\n",
       "  1591.29052734375,\n",
       "  457.4307861328125,\n",
       "  0.6885972023010254,\n",
       "  1425.97900390625,\n",
       "  444.2058410644531,\n",
       "  0.7940304279327393,\n",
       "  1630.96533203125,\n",
       "  543.3927612304688,\n",
       "  0.5835424661636353,\n",
       "  1392.9166259765625,\n",
       "  497.1055603027344,\n",
       "  0.7696180939674377,\n",
       "  1637.5777587890625,\n",
       "  622.7423095703125,\n",
       "  0.41547971963882446,\n",
       "  1366.466796875,\n",
       "  556.6177368164062,\n",
       "  0.8615410923957825,\n",
       "  1558.228271484375,\n",
       "  662.4171142578125,\n",
       "  0.7468500137329102,\n",
       "  1459.041259765625,\n",
       "  655.8046264648438,\n",
       "  0.7562357187271118,\n",
       "  1551.61572265625,\n",
       "  821.1162109375,\n",
       "  0.4312818944454193,\n",
       "  1459.041259765625,\n",
       "  801.27880859375,\n",
       "  0.6187584400177002,\n",
       "  1551.61572265625,\n",
       "  993.0402221679688,\n",
       "  0.3396550416946411,\n",
       "  1439.203857421875,\n",
       "  966.5903930664062,\n",
       "  0.6790436506271362],\n",
       " 'score': 2.4104461669921875,\n",
       " 'box': [1350.8701171875,\n",
       "  313.9403381347656,\n",
       "  275.8544921875,\n",
       "  677.1161804199219],\n",
       " 'idx': [0.0]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 48])\n"
     ]
    }
   ],
   "source": [
    "# How to use function\n",
    "output_3d=inferencealphaposeto3D_one(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': '00001.png',\n",
       " 'category_id': 1,\n",
       " 'pose_class': 'FallingDown_02',\n",
       " 'keypoints': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -97.96893722780383,\n",
       "  -4.742379032845212,\n",
       "  4.1372239011315415,\n",
       "  -34.01093909260862,\n",
       "  268.70586968369196,\n",
       "  -121.40576070155869,\n",
       "  -71.72193290420589,\n",
       "  709.8621679434074,\n",
       "  -107.84169791261655,\n",
       "  98.43087330682758,\n",
       "  4.035405071062647,\n",
       "  -5.780692450820728,\n",
       "  110.67299712624182,\n",
       "  311.1741643559309,\n",
       "  -137.26799283277106,\n",
       "  135.27674832847737,\n",
       "  761.7178265860889,\n",
       "  -133.5079092155104,\n",
       "  -14.734430609172723,\n",
       "  -223.53460645083175,\n",
       "  -7.834231579569881,\n",
       "  -42.40683964958995,\n",
       "  -474.9257487717407,\n",
       "  -49.64888095461019,\n",
       "  -127.23909158056924,\n",
       "  -433.45405912142064,\n",
       "  -134.79284931342528,\n",
       "  -92.76190020896219,\n",
       "  -464.67770156383165,\n",
       "  -130.72633033515206,\n",
       "  144.83576515707614,\n",
       "  -466.3122447167458,\n",
       "  -88.18001370934789,\n",
       "  257.1410066497345,\n",
       "  -278.58187682480616,\n",
       "  -123.41827637025659,\n",
       "  232.4732732530838,\n",
       "  -87.6819763537415,\n",
       "  -116.06111288072323,\n",
       "  -201.82564678721866,\n",
       "  -475.1026581284161,\n",
       "  -35.50817020297778,\n",
       "  -255.8056583738384,\n",
       "  -368.71679915644575,\n",
       "  101.32736174581487,\n",
       "  -308.65474030756104,\n",
       "  -245.64053175061238,\n",
       "  52.78702617455728],\n",
       " 'score': 2.4104461669921875,\n",
       " 'box': [1350.8701171875,\n",
       "  313.9403381347656,\n",
       "  275.8544921875,\n",
       "  677.1161804199219],\n",
       " 'idx': [0.0]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output is array of 3D Pose baseline with shape (51,)\n",
    "output_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_path='../inference_result_npy/taoyuan_angle2_3D_Original to taoyuan_angle1_3D_Original_transformationvalue.pickle'\n",
    "\n",
    "with open(trans_path, 'rb') as fp:\n",
    "    trans = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_transformation_single(trans,skeleton_):\n",
    "    \n",
    "    T=trans['T']\n",
    "    b=trans['b']\n",
    "    c=trans['c']\n",
    "    \n",
    "    skeleton=np.asarray(skeleton_['keypoints'])\n",
    "    skeleton = skeleton.reshape(-1, 3)\n",
    "    skeleton = (b * skeleton.dot(T)) + c\n",
    "    skeleton=skeleton.reshape(51,)\n",
    "    skeleton_['keypoints']=skeleton.tolist()\n",
    "    \n",
    "    return  skeleton_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': '00001.png',\n",
       " 'category_id': 1,\n",
       " 'pose_class': 'FallingDown_02',\n",
       " 'keypoints': [-7.276597748864106,\n",
       "  39.916814254366635,\n",
       "  17.265527389410295,\n",
       "  41.74932154696195,\n",
       "  23.234229694402895,\n",
       "  -50.11880153291253,\n",
       "  60.41747175415564,\n",
       "  286.0866247665738,\n",
       "  -11.72704140356365,\n",
       "  17.531047491415194,\n",
       "  641.4379424782436,\n",
       "  -149.34989559055035,\n",
       "  -55.34278341440974,\n",
       "  56.4666520766832,\n",
       "  85.92262113000967,\n",
       "  -9.657892230386816,\n",
       "  342.2523878639466,\n",
       "  83.99572153256734,\n",
       "  -79.62957592091999,\n",
       "  715.1769567817498,\n",
       "  -8.841930700062541,\n",
       "  32.81436428529889,\n",
       "  -143.98212091185522,\n",
       "  64.47917290411013,\n",
       "  106.1186379330458,\n",
       "  -344.22946718418456,\n",
       "  125.90239479458364,\n",
       "  203.29416873913337,\n",
       "  -299.90716222209704,\n",
       "  99.30511146742793,\n",
       "  186.2736019792124,\n",
       "  -322.43439153076673,\n",
       "  128.2299230395787,\n",
       "  33.26494076363154,\n",
       "  -305.3811402794964,\n",
       "  269.51883793413197,\n",
       "  -24.299457606069662,\n",
       "  -128.89831352236476,\n",
       "  317.83113031746456,\n",
       "  -39.498988004707414,\n",
       "  23.510617866964804,\n",
       "  251.8931611211034,\n",
       "  179.95262255105172,\n",
       "  -366.94375524089435,\n",
       "  10.887752716907592,\n",
       "  102.41175419923566,\n",
       "  -318.63998368031184,\n",
       "  -116.98594125538517,\n",
       "  148.12078863019735,\n",
       "  -212.06327913996768,\n",
       "  -158.97314216234543],\n",
       " 'score': 2.4104461669921875,\n",
       " 'box': [1350.8701171875,\n",
       "  313.9403381347656,\n",
       "  275.8544921875,\n",
       "  677.1161804199219],\n",
       " 'idx': [0.0]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_transformation_single(trans,output_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't run (just experiment and random code) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_class=set([])\n",
    "for a in new:\n",
    "    unique_class.add(a['pose_class'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open(\"../../../Falling Dataset/Thesis Experiment/data/taoyuan.json\") \n",
    "\n",
    "# returns JSON object as  \n",
    "# a dictionary \n",
    "data = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taoyuan_angle1=[]\n",
    "taoyuan_angle2=[]\n",
    "\n",
    "for a in data:\n",
    "    if ((a['pose_class']==\"FallingDown_01\") or (a['pose_class']==\"Standing_01\")):\n",
    "        taoyuan_angle1.append(a)\n",
    "    else:\n",
    "        taoyuan_angle2.append(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../Falling Dataset/Thesis Experiment/data/taoyuan_angle1.json\", 'w') as fp:\n",
    "    fp.write(json.dumps(taoyuan_angle1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../Falling Dataset/Thesis Experiment/data/taoyuan_angle2.json\", 'w') as fp:\n",
    "    fp.write(json.dumps(taoyuan_angle2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
